import jieba
import jieba.posseg as pseg
import multiprocessing
from multiprocessing import Pool, cpu_count

# ğŸŸ¢ æ™ºèƒ½å¯¼å…¥åŠ é€Ÿåº“
try:
    import jieba_fast as jieba
    import jieba_fast.posseg as pseg
except ImportError:
    pass


# ---------------------------------------------------------
# å¿…é¡»å®šä¹‰åœ¨é¡¶å±‚å‡½æ•°
# ---------------------------------------------------------

def _init_jieba_worker(custom_dict):
    """å­è¿›ç¨‹åˆå§‹åŒ–ï¼šåŠ è½½å­—å…¸"""
    # è®© jieba çŸ¥é“è¿™äº›è¯
    if custom_dict:
        for word in custom_dict:
            jieba.add_word(word, freq=20000)


def _worker_task(args):
    """
    å­è¿›ç¨‹æ‰§è¡Œçš„å…·ä½“ä»»åŠ¡
    args: (text_chunk, filter_type, stop_words, custom_dict)
    """
    # ğŸŸ¢ æ¥æ”¶ custom_dict
    text_chunk, filter_type, stop_words, custom_dict = args

    stop_words_set = set(stop_words)
    # ğŸŸ¢ å»ºç«‹ VIP åå• (è½¬å°å†™ä»¥åŒ¹é…)
    vip_words_set = set(w.strip().lower() for w in custom_dict) if custom_dict else set()

    valid_words = []

    if filter_type == "all":
        # å…¨æ–‡æ¨¡å¼
        words = jieba.cut(text_chunk, cut_all=False)
        for w in words:
            w = w.strip().lower()

            # ğŸŸ¢ VIP æ£€æŸ¥ï¼šå¦‚æœæ˜¯å¼ºåˆ¶ä¿ç•™è¯ï¼Œç›´æ¥é€šè¿‡
            if w in vip_words_set:
                valid_words.append(w)
                continue

            # æ™®é€šè§„åˆ™ï¼šå»ç©ºã€å»åœç”¨è¯ã€å»å•å­—
            if w and w not in stop_words_set and len(w) > 1:
                valid_words.append(w)
    else:
        # æ™ºèƒ½æå–æ¨¡å¼
        words = pseg.cut(text_chunk)

        for word_pair in words:
            w = word_pair.word
            flag = word_pair.flag
            w = w.strip().lower()

            # ğŸŸ¢ VIP æ£€æŸ¥ï¼šå¼ºåˆ¶ä¿ç•™è¯ï¼Œæ— è§†è¯æ€§ï¼Œæ— è§†åœç”¨è¯ï¼Œæ— è§†å•å­—é™åˆ¶
            if w in vip_words_set:
                valid_words.append(w)
                continue

            # æ™®é€šè§„åˆ™è¿‡æ»¤
            if not w or w in stop_words_set or len(w) < 2:
                continue

            keep = False
            if filter_type == "name":
                if flag.startswith('nr'): keep = True
            elif filter_type == "location":
                if flag.startswith('ns'): keep = True
            elif filter_type == "name_location":
                if flag.startswith('nr') or flag.startswith('ns'): keep = True
            elif filter_type == "org":
                if flag.startswith('nt'): keep = True

            if keep:
                valid_words.append(w)

    return valid_words


class ParallelTokenizer:
    """
    å¤šè¿›ç¨‹åˆ†è¯ç®¡ç†å™¨
    """

    @staticmethod
    def run_parallel(text, filter_type, custom_dict, stop_words):
        # 1. å‡†å¤‡æ•°æ®
        lines = text.split('\n')

        # 2. ç¡®å®šå¹¶è¡Œæ•°é‡
        num_cores = max(1, cpu_count())

        # 3. æ™ºèƒ½åˆ†å—
        chunk_size = len(lines) // num_cores + 1
        chunks = []
        current_chunk = []

        for line in lines:
            current_chunk.append(line)
            if len(current_chunk) >= chunk_size:
                chunks.append("\n".join(current_chunk))
                current_chunk = []
        if current_chunk:
            chunks.append("\n".join(current_chunk))

        # 4. å‡†å¤‡ä»»åŠ¡å‚æ•°
        # ğŸŸ¢ å…³é”®ä¿®æ”¹ï¼šæŠŠ custom_dict ä¹Ÿä¼ ç»™æ¯ä¸ªä»»åŠ¡ï¼Œç”¨äºåš VIP æ ¡éªŒ
        tasks = [(chunk, filter_type, stop_words, custom_dict) for chunk in chunks]

        # 5. å¯åŠ¨å¤šè¿›ç¨‹
        results = []
        with Pool(processes=num_cores, initializer=_init_jieba_worker, initargs=(custom_dict,)) as pool:
            raw_results = pool.map(_worker_task, tasks)

            for sub_list in raw_results:
                results.extend(sub_list)

        return results